{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":10126627,"sourceType":"datasetVersion","datasetId":6249169}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport pandas as pd\nimport numpy as np\nimport re\nfrom sklearn.model_selection import train_test_split\nimport gc\ngc.collect()\ntorch.cuda.empty_cache()\n!pip install datasets transformers","metadata":{"id":"EBwvH49xz4Pn","outputId":"bddb8f0c-aac2-4cfa-f484-d48943e93525","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T16:25:32.090619Z","iopub.execute_input":"2024-12-09T16:25:32.090871Z","iopub.status.idle":"2024-12-09T16:25:49.966495Z","shell.execute_reply.started":"2024-12-09T16:25:32.090832Z","shell.execute_reply":"2024-12-09T16:25:49.965461Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import OpenAIGPTConfig, OpenAIGPTModel\n\nconfiguration = OpenAIGPTConfig()\nmodel = OpenAIGPTModel(configuration)\nconfiguration = model.config\nprint(configuration)","metadata":{"id":"MUrT398Eggkg","outputId":"d63fe201-99ad-483b-dc14-b1c13431776c","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T16:25:49.969054Z","iopub.execute_input":"2024-12-09T16:25:49.970254Z","iopub.status.idle":"2024-12-09T16:26:14.943820Z","shell.execute_reply.started":"2024-12-09T16:25:49.970219Z","shell.execute_reply":"2024-12-09T16:26:14.942806Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\nds = load_dataset(\"nlpcloud/instructions-dataset-adapted-from-stanford-alpaca-for-gpt-j\", split=\"train\")","metadata":{"id":"swBmUjwQ55Bo","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T16:26:14.945029Z","iopub.execute_input":"2024-12-09T16:26:14.945622Z","iopub.status.idle":"2024-12-09T16:26:17.576350Z","shell.execute_reply.started":"2024-12-09T16:26:14.945588Z","shell.execute_reply":"2024-12-09T16:26:17.575716Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import GPT2Tokenizer\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")","metadata":{"id":"V-3wI51vk2ww","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T16:26:17.577277Z","iopub.execute_input":"2024-12-09T16:26:17.577792Z","iopub.status.idle":"2024-12-09T16:26:18.566003Z","shell.execute_reply.started":"2024-12-09T16:26:17.577762Z","shell.execute_reply":"2024-12-09T16:26:18.565038Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hyperparameters","metadata":{"id":"vrykN73he2bA"}},{"cell_type":"code","source":"class GPTConfig:\n# The original GPT model featured 12 layers, 768 hidden units, and 12 attention heads, totaling 117 million parameters.\n# Use only a half of the above hyperparameters\n  block_size = 512\n  batch_size = 32\n  max_iters = 500\n  eval_interval = 100\n  learning_rate = 3e-4\n  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n  eval_iters = 200\n  n_embd = 384\n  n_head = 6\n  n_layer = 6\n  dropout = 0.2","metadata":{"id":"KWKRSnE-e7J7","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T16:31:06.634798Z","iopub.execute_input":"2024-12-09T16:31:06.635462Z","iopub.status.idle":"2024-12-09T16:31:06.639858Z","shell.execute_reply.started":"2024-12-09T16:31:06.635428Z","shell.execute_reply":"2024-12-09T16:31:06.638993Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocess the data","metadata":{"id":"Vbt4nP93P7ld"}},{"cell_type":"code","source":"df = pd.DataFrame(ds)\ntexts = []\nfor i in range(2, len(df), 3):\n    question = re.sub(r'\\\\[ntr]', '', df['text'][i-2]).strip()\n    answer = re.sub(r'\\\\[ntr]', '', df['text'][i-1]).strip()\n    question = re.sub(r'\\\\u', ' ', question)\n    answer = re.sub(r'\\\\u', ' ', answer)\n\n    texts.append({\n        'question': question,\n        'answer': answer\n    })\nprint(len(texts))\ntexts[:5]\n","metadata":{"id":"BYMHhwX5P_r6","outputId":"d7626d87-c5d9-4730-d50e-89c06f55c920","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T16:31:06.641352Z","iopub.execute_input":"2024-12-09T16:31:06.641653Z","iopub.status.idle":"2024-12-09T16:31:09.746478Z","shell.execute_reply.started":"2024-12-09T16:31:06.641610Z","shell.execute_reply":"2024-12-09T16:31:09.745704Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_single_text(texts):\n    full_text = \"\"\n    for item in texts:\n        # Add special tokens to mark start and end of QA pairs\n        full_text += f\"<|startoftext|>Question: {item['question']}\\nAnswer: {item['answer']}<|endoftext|>\\n\\n\"\n    return full_text\n\n# Add special tokens to tokenizer\nspecial_tokens = {\n    'pad_token': '<|pad|>',\n    'bos_token': '<|startoftext|>',\n    'eos_token': '<|endoftext|>'\n}\ntokenizer.add_special_tokens(special_tokens)\n\n# Create the full text\nfull_text = create_single_text(texts)\n\n# Tokenize the entire text\ntokens = tokenizer.encode(full_text)\ntokens = torch.tensor(tokens, dtype=torch.long)\ntokens.shape","metadata":{"id":"sAyBYUrf2kIr","outputId":"b8120432-885b-4ed7-c265-f515857f0da2","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T16:31:09.748174Z","iopub.execute_input":"2024-12-09T16:31:09.748846Z","iopub.status.idle":"2024-12-09T16:31:25.232530Z","shell.execute_reply.started":"2024-12-09T16:31:09.748805Z","shell.execute_reply":"2024-12-09T16:31:25.231688Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(tokens[:60])","metadata":{"id":"xSzt_jQKGfP1","outputId":"cdfd46d1-813f-4108-b954-782b7af120b3","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T16:31:25.233521Z","iopub.execute_input":"2024-12-09T16:31:25.233772Z","iopub.status.idle":"2024-12-09T16:31:25.239473Z","shell.execute_reply.started":"2024-12-09T16:31:25.233748Z","shell.execute_reply":"2024-12-09T16:31:25.238569Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nfrom torch.utils.data import Dataset, DataLoader\n\nclass GPTDataset(Dataset):\n    def __init__(self, tokens, block_size):\n        self.tokens = tokens\n        self.block_size = block_size\n\n    def __len__(self):\n        return len(self.tokens) - self.block_size\n\n    def __getitem__(self, idx):\n        chunk = self.tokens[idx:idx + self.block_size + 1]\n        if len(chunk) < self.block_size + 1:\n            padding = torch.full((self.block_size + 1 - len(chunk),), tokenizer.pad_token_id, dtype=torch.long)\n            chunk = torch.cat([chunk, padding])\n        x = chunk[:self.block_size]\n        y = chunk[1:self.block_size + 1]\n        return x, y\n\n# Create dataset and dataloader\ntrain_dataset = GPTDataset(tokens, GPTConfig.block_size)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n'''\ndef get_batch():\n    # generate a small batch of data of inputs x and targets y\n    ix = torch.randint(len(tokens) - GPTConfig.block_size, (GPTConfig.batch_size,))\n    x = torch.stack([tokens[i:i+GPTConfig.block_size] for i in ix])\n    y = torch.stack([tokens[i+1:i+GPTConfig.block_size+1] for i in ix])\n    x, y = x.to(GPTConfig.device), y.to(GPTConfig.device)\n    return x, y","metadata":{"id":"a9mrl9vPrlHl","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T16:32:16.680914Z","iopub.execute_input":"2024-12-09T16:32:16.681250Z","iopub.status.idle":"2024-12-09T16:32:16.687817Z","shell.execute_reply.started":"2024-12-09T16:32:16.681219Z","shell.execute_reply":"2024-12-09T16:32:16.686180Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Attention from Scratch","metadata":{"id":"BAbO7EpLsPhA"}},{"cell_type":"code","source":"class Attention(torch.nn.Module):\n    def __init__(self, embed, head_size):\n        super().__init__()\n        self.q = torch.nn.Linear(embed, head_size, bias=False)\n        self.k = torch.nn.Linear(embed, head_size, bias=False)\n        self.v = torch.nn.Linear(embed, head_size, bias=False)\n        # Register the triangular mask buffer\n        self.register_buffer('tril', torch.tril(torch.ones(GPTConfig.block_size, GPTConfig.block_size)))\n\n    def forward(self, x):\n        B, T, C = x.shape  # batch, time, channels\n        Q = self.q(x)  # shape: batch_size, num_tokens, head_size\n        K = self.k(x)  # shape: batch_size, num_tokens, head_size\n        V = self.v(x)  # shape: batch_size, num_tokens, head_size\n\n        temp = torch.matmul(Q, K.transpose(-2, -1)) / (K.shape[-1] ** 0.5)  # shape: batch_size, num_tokens, num_tokens\n        # Apply causal mask\n        temp = temp.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        attn = F.softmax(temp, dim=-1)  # shape: batch_size, num_tokens, num_tokens\n        return torch.matmul(attn, V)  # shape: batch_size, num_tokens, head_size","metadata":{"id":"-GUHh7XPh32U","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T16:31:25.251704Z","iopub.execute_input":"2024-12-09T16:31:25.252044Z","iopub.status.idle":"2024-12-09T16:31:25.264729Z","shell.execute_reply.started":"2024-12-09T16:31:25.252008Z","shell.execute_reply":"2024-12-09T16:31:25.264093Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MultiHeadAttention(torch.nn.Module):\n  def __init__(self, embed, num_heads):\n    super().__init__()\n    head_size = embed // num_heads\n    self.heads = torch.nn.ModuleList([Attention(embed, head_size) for _ in range(num_heads)])\n    self.proj = torch.nn.Linear(head_size * num_heads, embed)\n    self.dropout = torch.nn.Dropout(GPTConfig.dropout)\n  def forward(self, x):\n    out = torch.cat([h(x) for h in self.heads], dim=-1)\n    return self.dropout(self.proj(out))","metadata":{"id":"aaK9abGgEmcw","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T16:31:25.265632Z","iopub.execute_input":"2024-12-09T16:31:25.265863Z","iopub.status.idle":"2024-12-09T16:31:25.285210Z","shell.execute_reply.started":"2024-12-09T16:31:25.265840Z","shell.execute_reply":"2024-12-09T16:31:25.284407Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Transformer Block from Scratch (inspired by GPT structure mentioned by GeeksForGeeks)","metadata":{"id":"XEZn_n7H1LNv"}},{"cell_type":"code","source":"class TransformerBlock(torch.nn.Module):\n  def __init__(self, embed, num_heads):\n    super().__init__()\n    self.ln1 = torch.nn.LayerNorm(embed) # Normalizes across feature dimension embed\n    self.ln2 = torch.nn.LayerNorm(embed)\n    self.attn = MultiHeadAttention(embed, num_heads)\n    self.ffn = torch.nn.Sequential(\n        torch.nn.Linear(embed, 4 * embed),\n        torch.nn.ReLU(), # Original is GELU, but ReLU for simplicity\n        torch.nn.Linear(4 * embed, embed),\n        torch.nn.Dropout(GPTConfig.dropout),\n    )\n    self.do1 = torch.nn.Dropout(GPTConfig.dropout)\n    self.do2 = torch.nn.Dropout(GPTConfig.dropout)\n  def forward(self, x):\n    x = x + self.do1(self.attn(self.ln1(x)))\n    x = x + self.do2(self.ffn(self.ln2(x)))\n    return x\n","metadata":{"id":"N309WYvazGru","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T16:31:25.286156Z","iopub.execute_input":"2024-12-09T16:31:25.286438Z","iopub.status.idle":"2024-12-09T16:31:25.299260Z","shell.execute_reply.started":"2024-12-09T16:31:25.286413Z","shell.execute_reply":"2024-12-09T16:31:25.298696Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GPT(torch.nn.Module):\n  def __init__(self, vocab_size, embed, num_heads, num_blocks):\n    super().__init__()\n    self.token_embedding = torch.nn.Embedding(vocab_size, embed)\n    self.position_embedding = torch.nn.Embedding(GPTConfig.block_size, embed)\n    self.blocks = torch.nn.Sequential(*[TransformerBlock(embed, num_heads) for _ in range(num_blocks)])\n    self.ln = torch.nn.LayerNorm(embed)\n    self.do = torch.nn.Dropout(GPTConfig.dropout)\n    self.lm = torch.nn.Linear(embed, vocab_size) # Hidden state to output logits\n  def forward(self, x, targets=None):\n    if x.shape[1] > GPTConfig.block_size:\n        x = x[:, -GPTConfig.block_size:]  # Keep only the last block_size tokens\n    tok_emb = self.token_embedding(x) # (batch_size, block_size(len_sequence), embed)\n    pos_ids = torch.arange(0, x.shape[1], device=x.device).unsqueeze(0) #(1, block_size)\n    pos_emb = self.position_embedding(pos_ids) # (1, block_size, embed)\n    x = self.do(tok_emb + pos_emb) # (batch_size, block_size, embed)\n    x = self.blocks(x) # (batch_size, block_size, embed)\n    x = self.ln(x) # (batch_size, block_size, embed)\n    logits = self.lm(x) # (batch_size, block_size, vocab_size)\n    loss = None\n    if targets is not None:\n      loss_fn = nn.CrossEntropyLoss()\n      loss = loss_fn(logits.view(-1, logits.shape[-1]), targets.view(-1)) # logits.shape: (batch_size, block_size, vocab_size) -> (batch_size * block_size, vocab_size)\n                                                                          # targets.shape: (batch_size, block_size) -> (batch_size * block_size)\n    return logits, loss\n  def generate(self, input, max_new_tokens, temperature=1.0, top_k=None):\n    self.eval()\n    with torch.no_grad():\n      for _ in range(max_new_tokens):\n          context = input[:, -GPTConfig.block_size:]\n          logits, loss = self(context)  # logits shape: (batch_size, seq_len, vocab_size)\n          logits = logits[:, -1, :] / temperature  # Take logits of the last token in the sequence (batch_size, vocab_size)\n          if top_k is not None:\n              v, _ = torch.topk(logits, top_k)\n              logits[logits < v[:, [-1]]] = float('-inf')\n          probs = F.softmax(logits, dim=-1)  # (batch_size, vocab_size)\n          idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n          input = torch.cat((input, idx_next), dim=1)  # (batch_size, seq_len + 1)\n          input = input[:, -GPTConfig.block_size:]  # Keep only the most recent `block_size` tokens\n\n    return input\n","metadata":{"id":"2vV5Ib3l4203","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T16:31:25.300219Z","iopub.execute_input":"2024-12-09T16:31:25.300548Z","iopub.status.idle":"2024-12-09T16:31:25.317145Z","shell.execute_reply.started":"2024-12-09T16:31:25.300490Z","shell.execute_reply":"2024-12-09T16:31:25.316555Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = GPT(len(tokenizer), GPTConfig.n_embd, GPTConfig.n_head, GPTConfig.n_layer).to(GPTConfig.device)\ntorch.save(model.state_dict(), \"/kaggle/working/gpt_model.pth\")\nmodel.load_state_dict(torch.load(\"/kaggle/working/gpt_model.pth\"))\n\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs\")\n    model = nn.DataParallel(model)\nmodel = model.to(GPTConfig.device)","metadata":{"id":"7uCx5ci9hSzH","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T16:31:25.317993Z","iopub.execute_input":"2024-12-09T16:31:25.318298Z","iopub.status.idle":"2024-12-09T16:31:26.767387Z","shell.execute_reply.started":"2024-12-09T16:31:25.318262Z","shell.execute_reply":"2024-12-09T16:31:26.766548Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=GPTConfig.learning_rate)\nfor epoch in range(GPTConfig.max_iters): #GPTConfig.max_iters\n    gc.collect()\n    torch.cuda.empty_cache()\n    model.train()\n    epoch_loss = 0\n    '''\n    for x, y in train_loader:\n        x, y = x.to(GPTConfig.device), y.to(GPTConfig.device)\n    '''\n    for i in range(GPTConfig.eval_interval): #GPTConfig.eval_interval\n        x, y = get_batch()\n        \n        # Clear previous gradients\n        optimizer.zero_grad(set_to_none=True)\n        \n        # Forward pass and loss calculation\n        logits, loss = model(x, targets=y)\n        \n        # Normalize loss for distributed training scenarios\n        if torch.is_tensor(loss):\n            # Handle multi-GPU scenarios\n            if loss.dim() > 0:\n                loss = loss.mean()\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        \n        # Track epoch loss\n        epoch_loss += loss.item()\n    \n    # Calculate average loss\n    avg_loss = epoch_loss / GPTConfig.eval_interval #GPTConfig.eval_interval\n    print(f\"Epoch {epoch+1}/{GPTConfig.max_iters}: Loss = {avg_loss}\")\n    \n    # Save model weights with support for different model wrappers\n    if isinstance(model, (nn.DataParallel, nn.parallel.DistributedDataParallel)):\n        torch.save(model.module.state_dict(), \"/kaggle/working/gpt_model.pth\")\n    else:\n        torch.save(model.state_dict(), \"/kaggle/working/gpt_model.pth\")","metadata":{"id":"I8vrkVEGzt5O","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T16:38:25.487058Z","iopub.execute_input":"2024-12-09T16:38:25.487456Z","iopub.status.idle":"2024-12-09T16:38:39.819445Z","shell.execute_reply.started":"2024-12-09T16:38:25.487427Z","shell.execute_reply":"2024-12-09T16:38:39.818415Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\nimport os\nos.chdir(r'/kaggle/working')\nfrom IPython.display import FileLink\nFileLink(r'gpt_model.pth')\n\ndef generate_response(model, question, max_tokens=100):\n    # Access the original model if wrapped in DataParallel\n    if isinstance(model, torch.nn.DataParallel):\n        model = model.module\n\n    model.eval()\n    input_text = f\"<|startoftext|>Question: {question}\\nAnswer:\"\n    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(GPTConfig.device)\n\n    with torch.no_grad():\n        output_ids = model.generate(\n            input_ids,\n            max_new_tokens=max_tokens,\n            temperature=0.7,\n            top_k=50\n        )\n\n    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=False)\n    answer = generated_text.split(\"Answer:\")[-1].strip()\n    end_token_pos = answer.find(\"<|endoftext|>\")\n    if end_token_pos != -1:\n        answer = answer[:end_token_pos].strip()\n    return answer\n","metadata":{"id":"aV-E0r56Zbhc","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T16:38:56.956622Z","iopub.execute_input":"2024-12-09T16:38:56.956967Z","iopub.status.idle":"2024-12-09T16:38:58.495919Z","shell.execute_reply.started":"2024-12-09T16:38:56.956936Z","shell.execute_reply":"2024-12-09T16:38:58.495040Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_question = \"What are the three primary colors?\"\nresponse = generate_response(model, sample_question)\nprint(f\"Question: {sample_question}\")\nprint(f\"Answer: {response}\")","metadata":{"id":"wDV9Jc82F2gF","trusted":true},"outputs":[],"execution_count":null}]}